% Chapter 3

\chapter{Computational experimentation} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 3. \emph{Computational experimentation}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

Nowadays Scientific Computing is one the most important tools for Science. Computational implementation became crucial when the problem cannot be solved by traditional experimental or theoretical means. There are a number or reasons why this might happen, for example whenever experimentation may be dangerous, to expensive or time-consuming.

Stochastic topology has embraced the methods of scientific computing to provide a better insight of complex phenomena. For theoretical develop it has allowed, for example, to verify if the established conditions in a probabilistic model are sharp enough (\cite{Meshulam13}). 

Computational develop mixed with a strong probability background has demonstrated to be efficient and powerful. Algorithms based in random sampling has provided state-of-the-art techniques due to their great degree of flexibility and reliability.

The PageRank algorithm, was the first method used by Google to order search results \cite{pageRank}. It outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page.

In motion planning, the use of Rapidly-exploring random trees (RRTs) are one of the most successful algorithms \cite{Alcazar15}. Problems in motion planning consist on finding a collision-free path that connects an initial configuration of geometric bodies to a final goal configuration. A RRT is a rooted tree that grows from a starting configuration by using random samples from the search space. 

In Bayesian Inference there is a family of algorithms, known collectively as Markov Chain Monte Carlo (MCMC), that allow us to approximate the posterior distribution as calculated by Bayes' Theorem.

In this chapter we use the outlined probabilistic material to accomplish an efficient implementation of rigidity phenomenon in graphs. This resulted in a computational tool that provide a richer insight of the concept. We describe our results with their technical difficulties and the actions taken to endure them.

All the computational experimentation was develop in \texttt{python}, it provide simple and flexible representations of networks as well as clear and concise expressions of network algorithms. The \texttt{\href{https://networkx.github.io/}{NetworkX}} library allowed to create and modify graphs as objects (dictionaries) and provided more features such as numerical linear algebra and drawing.

\section{Simulating graphs in $G(n,p)$}

There is a direct algorithm to obtain a graph in $G(n,p)$, it simulate a $Bernoulli(p)$ r.v. for each of the $\frac{n(n-1)}{2}$ possible edges. Thus, it runs in $O(n^2)$ time. It is possible to do faster algorithms for small values of $p$ that runs in $O(n + m)$ time, where $m$ is the expected number of edges, which equals $\frac{pn(n - 1)}{2}$ \cite{fastER}.

In the figure \ref{fig:ErdosRenyi10} appears a set of graphs obtained with such algorithm, fixing $n=10$ an varying the parameter $p$.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.85]{Figures/ER-10.png}
	\caption{Erdös-Rényi random graphs with $n$ fixed and varying $p$}
	\label{fig:ErdosRenyi10}
\end{figure}

Figure \ref{fig:tiemposER} show the execution times varying $n$
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.8]{Python/Figures/Times-ER.png}
	\caption{Execution times varying $n$. Both normal and logarithmic scale appear in the figure}
	\label{fig:tiemposER}
\end{figure}

\section{Rigid expansions algorithm}
A priory, the algorithm to determine a rigid expansion is supposed to be executed in a large amount of time. As the definition let us see, it depends on the size of $ G $, and it depends exponentially on the size of $A$; it must check among all the possible subsets of $A$, that is $2^{m}$ verifications, where $|A|=m$. Thus, it is important to do some optimization to the algorithms and evaluate when this have more impact in the expected execution time according to the parameters taken.

The following is the straightforward algorithm for rigid expansions.

\begin{cajita}
\textbf{Rigid Expansions Algorithm} \hfill \break

\begin{tabular}{ l l }
\texttt{Input:} &  \texttt{Random graph $G$ (dictionary),} \\
                &  \texttt{set of vertices $A$ (array).}\\
\texttt{Output:} & \texttt{Set of vertices obtained after expanding $A$ (array)} \\
\end{tabular}
\begin{enumerate}
\item Initialize N as empty (the set of new vertices)
\item For every $B$, subset of A:\hfill \break
\hphantom{12} If $\bigcap\limits_{b\in B} N(b) = v$ and $v\not\in A\cup N$: \hfill \break
\hphantom{1234} Add $v$ to $N$
\item If $N$ is not empty: \hfill \break
\hphantom{12} Replace $A$ by $A\cup N$ and return to step 1. \hfill \break
      Otherwise:\hfill \break
\hphantom{12} Return $A$
\end{enumerate}
\end{cajita}

In step two the iterations were indexed by \texttt{generators}. These are iterators in \texttt{python} were you can only iterate over once, so they do not store all the values in memory.

We proposed the following optimizations:

\begin{enumerate}
\item \textbf{Consideration of isolated vertices and leaves.} None of the isolated vertices in $A$ have any influence in rigid expansions, so they should not be consider. Also, whenever $A$ contains a leave is convenient to ignore them; the unique neighbor of a leave, which we will call \textit{petioles} should be automatically added in the first expansion an then it does not contribute in uniquely determine new vertices. This means that the input should be replace with:
$$A' = A - \{v: deg(v)\leq 1 \} \cup \{u: \exists x, N(x)=\{u\}\} $$
and add them again by the end of the expansions.
\item \textbf{Relative size of $A$.} In Step 2, if $A$ big enough is faster to check if a vertex outside of $A$ can be uniquely determinate by a subset of $A$. This can reduce dramatically the execution time when $p$ is small; it reduce the size of revisions by taking only the \textit{effective} part of $A$, this is convenient to do whenever
$$k\cdot log(2) > log(n-k) + (kp)\cdot log(2)$$
were $k$ is the size of $A$

\item \textbf{Restriction to effective subsets}. Calculations in Chapter 2 showed that there are subsets which are more likely to generate rigid expansions than others. This depend on the parameters of the space and the size of the subsets. If we restrict to these effective subsets we can reduce the number of verifications.
\end{enumerate}

With these optimizations we obtain the following algorithm

\begin{cajita}
\textbf{Optimized Rigid Expansions Algorithm} \hfill \break

\begin{tabular}{ l l }
\texttt{Input:} &  \texttt{Random graph $G$ (dictionary),} \\
                &  \texttt{set of vertices $A$ (array),} \\
                &  \texttt{$n$(int) and $p$(float)} \\
\texttt{Output:} & \texttt{Set of vertices obtained after expanding $A$ (array)} \\
\end{tabular}
\begin{enumerate}
\item Replace $A$ by $A'$
\item Initialize $N$ as petioles of $A$ (the set of new vertices)
\item Calculate the range of effective subsets.\hfill \break
If $k\cdot log(2) > log(n-k) + (kp)\cdot log(2)$: \hfill \break
\hphantom{12} For every $v\in V-A$:\hfill \break
\hphantom{1234} Take $C = A\cap N(v)$ and for every $B$, effective subset of C:\hfill \break
\hphantom{123456} If $\bigcap\limits_{b\in B} N(b) = v$ and $v\not\in A\cup N$: \hfill \break
\hphantom{12341234} Add $v$ to $N$

Otherwise:\hfill \break
\hphantom{12} For every $B$, effective subset of A:\hfill \break
\hphantom{1234} If $\bigcap\limits_{b\in B} N(b) = v$ and $v\not\in A\cup N$: \hfill \break
\hphantom{123456} Add $v$ to $N$

\item If $N$ is not empty: \hfill \break
\hphantom{12} Replace $A$ by $A\cup N$, initialize $N$ as empty and return to step 3. \hfill \break
      Otherwise:\hfill \break
\hphantom{12} Return $A\cup\{v: deg(v)\leq 1 \}$
\end{enumerate}
\end{cajita}

\section{Conclusions}

%Conclusiones sobre tu experimentación computacional.






